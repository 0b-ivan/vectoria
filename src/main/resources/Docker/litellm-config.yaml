model_list:
  - model_name: qwen-local-chat
    litellm_params:
      # Modellname aus Sicht von LiteLLM â€“ kann irgendwas sein
      model: gpt-3.5-turbo
      # llama.cpp-Server (OpenAI-kompatible API)
      api_base: "http://llama:8000/v1"
      api_key: "dummy"
      # WICHTIG: KEIN custom_llm_provider hier

  - model_name: qwen-local-embed
    litellm_params:
      model: gpt-3.5-turbo
      api_base: "http://llama:8000/v1"
      api_key: "dummy"
