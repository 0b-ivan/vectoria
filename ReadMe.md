# Vectoria â€“ Local RAG System with Java Spring + llama (WIP)

Vectoria ist ein vollstÃ¤ndig lokales **Retrieval-Augmented-Generation System (RAG)**, implementiert in  
**Java Spring Boot** mit einem **lokalen LLM Ã¼ber llama.cpp** gesteuert via **LiteLLM**.

Das System ermÃ¶glicht:

- Upload von Dokumenten
- automatisches Chunking
- Generierung von Embeddings Ã¼ber Chat (kein /embeddings-Endpoint!)
- semantische Suche per Cosine Similarity
- Anfragebeantwortung durch RAG-Pipeline
- schlankes Frontend mit Dokumentauswahl + Fragestellung

---

## ðŸš€ Features

- ðŸŸ¢ **100% lokal** â€“ keine Cloud, keine externen APIs
- ðŸ§© **Chat-basierte Embeddings** (Qwen/llama.cpp â†’ LiteLLM â†’ /chat/completions)
- ðŸ“š **Dokument-Upload & Chunking**
- ðŸ”Ž **Similarity Search** mit Cosine Similarity Ã¼ber Postgres JSONB
- ðŸ¤– **RAG Prompt Generation** fÃ¼r hochwertige Antworten
- ðŸŒ Frontend fÃ¼r Benutzerabfragen
- ðŸ”§ Erweiterbar, modular & klar strukturiert

---

## ðŸ— ArchitekturÃ¼berblick

### Backend (Spring Boot)

| Komponente        | Beschreibung |
|------------------|--------------|
| **ChunkController** | Upload, Chunking, Dokumentliste |
| **EmbeddingService** | Generiert Embeddings fÃ¼r Dokument-Chunks via Chat |
| **RagService** | Frage-Embedding, Similarity Search, Prompt-Erstellung, LLM-Query |
| **LlmClient** | `/chat/completions` API; sowohl Chat-Antworten als auch Embeddings via Chat |
| **PostgreSQL** | Persistiert Chunks + deren Embeddings (JSONB) |

### LLM Layer

Vectoria nutzt *lokale Modelle* wie Qwen oder LLaMA Ã¼ber **llama.cpp**.  
LiteLLM dient als **OpenAI-kompatible Chat-API**, jedoch nur fÃ¼r `/chat/completions`.

**Keine Verwendung von `/embeddings`!**

Embeddings werden so generiert: -> WIP

Das Ergebnis wird geparst und als `embedding_json` gespeichert.

---

## ðŸ—„ Datenbank-Schema

### `document_chunks`

```sql
CREATE TABLE document_chunks (
    id BIGSERIAL PRIMARY KEY,
    document_id TEXT NOT NULL,
    chunk_index INT NOT NULL,
    content TEXT NOT NULL,
    embedding_json JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

## LiteLLM Konfiguration (erforderlich)
litellm-config:
```yaml
model_list:
  - model_name: qwen-local-chat
    litellm_params:
      # Modellname aus Sicht von LiteLLM â€“ kann irgendwas sein
      model: gpt-3.5-turbo
      # llama.cpp-Server (OpenAI-kompatible API)
      api_base: "http://llama:8000/v1"
      api_key: "dummy"
      # WICHTIG: KEIN custom_llm_provider hier

  - model_name: qwen-local-embed
    litellm_params:
      model: gpt-3.5-turbo
      api_base: "http://llama:8000/v1"
      api_key: "dummy"
```

## RAG Ablauf
1.	Benutzer stellt Frage
2.	Frage wird eingebettet (Chat-Hack)
3.	DB lÃ¤dt alle Chunk-Embeddings
4.	Cosine Similarity â†’ Top-K Chunks
5.	Prompt wird mit Kontext gebaut
6.	LLM antwortet via LlmClient.chat()
7.	Antwort und verwendete Chunks werden zurÃ¼ckgegeben

## Backend starten

```bash
mvn spring-boot:run
```

